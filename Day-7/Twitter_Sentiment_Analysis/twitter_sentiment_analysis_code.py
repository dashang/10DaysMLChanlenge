# -*- coding: utf-8 -*-
"""Twitter_Sentiment_Analysis_Code.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1WDOG_nS5eB8wzeVHRijPP4XEhobAgptY
"""

import pandas as pd
import numpy as np
import nltk

pwd

cd '/content/drive/My Drive/Deep_Learning /Twitter_Sentiment_Analysis'

ls

df = pd.read_csv('train.csv')

df.shape

df.head(n=15)

nltk.download('stopwords')

nltk.download('punkt')

print(len(df[df.label == 0]), 'Non-Hatred Tweets')
print(len(df[df.label == 1]), 'Hatred Tweets')

"""# Cleaning data"""

from nltk.tokenize import sent_tokenize,word_tokenize
from nltk.tokenize import RegexpTokenizer
from nltk.corpus import stopwords
from nltk.stem.snowball import PorterStemmer,SnowballStemmer
from nltk.stem.lancaster import LancasterStemmer

tokenizer = RegexpTokenizer("[a-zA-Z]+")
ps = PorterStemmer()
sw = set(stopwords.words('english'))

def cleanTweet(text):
  text = text.lower()
  text = text.replace("<br /><br />"," ")
  text = text.replace("@user","")
  tokens = tokenizer.tokenize(text)
  new_tokens = [token for token in tokens if token not in sw]
  stemmed_tokens = [ps.stem(token) for token in new_tokens]
  cleaned_review = ' '.join(stemmed_tokens)
  return cleaned_review

df['cleaned_tweet'] = df.tweet.apply(cleanTweet)

df.head(n=15)

"""# Creating Word Cloud for Hated Tweets"""

from wordcloud import WordCloud

hated_words = " ".join(df[df['label']==1].cleaned_tweet)
print(hated_words)

import matplotlib.pyplot as plt

wordcloud = WordCloud(height=4000, width=4000, stopwords=sw, background_color='white')
wordcloud = wordcloud.generate(hated_words)
plt.imshow(wordcloud)
plt.axis('off')
plt.show()

"""# Creating Word Cloud for all Tweets"""

all_words = " ".join(df.cleaned_tweet)

wordcloud2 = WordCloud(height=5000, width=5000, stopwords=sw, background_color='white')
wordcloud2 = wordcloud2.generate(all_words)
plt.imshow(wordcloud2)
plt.axis('off')
plt.show()

countt = [29720,2242]

from collections import Counter

counter = Counter(df['label'])
labels = counter.keys()
counts = counter.values()
indexes = np.arange(len(labels))
width = 0.7
plt.bar(indexes,counts, width)
plt.xticks(indexes + width * 0.5, labels)
plt.title('Counts of 0 and 1')
plt.show()

"""# Building a Common Corpus"""

from sklearn.feature_extraction.text import CountVectorizer
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.metrics import confusion_matrix, classification_report, f1_score

corpus = []
for i in range(0,31962):
    corpus.append(df['cleaned_tweet'][i])

cv = CountVectorizer(stop_words=sw)
cv.fit(corpus)

X = cv.transform(corpus).toarray()
y = df.iloc[:,1].values

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)

classifier1 = LogisticRegression()
classifier1.fit(X_train, y_train)
y_pred = classifier1.predict(X_test)
y_prob = classifier1.predict_proba(X_test)

print(f1_score(y_test, y_pred))
print(classification_report(y_test, y_pred))
print(confusion_matrix(y_test, y_pred))

from sklearn.metrics import accuracy_score

accuracy_score(y_test, y_pred)

